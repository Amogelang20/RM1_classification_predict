{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Predict - Climate Change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Table of contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Problem identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# background and problem statement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### What data do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# description of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Start experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Install necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "# !pip install comet_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\programdata\\anaconda3\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: pandas>=0.15.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from seaborn) (1.0.3)\n",
      "Requirement already satisfied: scipy>=0.14.0 in c:\\users\\myne\\appdata\\roaming\\python\\python37\\site-packages (from seaborn) (1.4.1)\n",
      "Requirement already satisfied: matplotlib>=1.4.3 in c:\\users\\myne\\appdata\\roaming\\python\\python37\\site-packages (from seaborn) (3.2.1)\n",
      "Requirement already satisfied: numpy>=1.9.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from seaborn) (1.18.5)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.15.2->seaborn) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.15.2->seaborn) (2019.3)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.3->seaborn) (2.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.3->seaborn) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.3->seaborn) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas>=0.15.2->seaborn) (1.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.3->seaborn) (41.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\programdata\\anaconda3\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\myne\\appdata\\roaming\\python\\python37\\site-packages (from wordcloud) (3.2.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (1.18.5)\n",
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (6.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.4.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.1->matplotlib->wordcloud) (1.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib->wordcloud) (41.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\programdata\\anaconda3\\lib\\site-packages (0.5.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspellchecker in c:\\programdata\\anaconda3\\lib\\site-packages (0.5.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ftfy in c:\\programdata\\anaconda3\\lib\\site-packages (5.7)\n",
      "Requirement already satisfied: wcwidth in c:\\programdata\\anaconda3\\lib\\site-packages (from ftfy) (0.1.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from comet_ml import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Setting the API key (saved as environment variable)\n",
    "# experiment = Experiment(api_key=\"upOwchWrd7H1e6VEnWKW7PSvz\", project_name=\"classification-predict\", workspace=\"team-rm1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "# Basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Libraries for visualisations\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# Libraries for preprocessing\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.probability import FreqDist\n",
    "import emoji\n",
    "from ftfy import fix_text\n",
    "from spellchecker import SpellChecker \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import itertools\n",
    "\n",
    "# Libraries for fitting models\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Libary for splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Library for hyperparameter optimisation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Libraries for evaluating model accuracy\n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Libraries for saving models\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('https://raw.githubusercontent.com/Amogelang20/RM1_classification_predict/dev/test.csv')\n",
    "df_train = pd.read_csv('https://raw.githubusercontent.com/Amogelang20/RM1_classification_predict/dev/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_train.set_index('tweetid',inplace = True)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_test.set_index('tweetid',inplace = True)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let us first get quick overview of the dataset we will be working with throughout the notebook. Below is a list of all columns with their data types and the number of non-null values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There are only two columns present in the training dataset; the label variable we want to classify to (`'sentiment'`), and the feature we will use to make this classification (`'message'`). We initially had a third column (`'tweetid'`) but set this column to be the index of the dataset.  The dataset contains no null entries, and the data types for (`'sentiment'`) and (`'message'`) are integer and object respectively. \n",
    "\n",
    "Next we will have a look at the different labels we will be classifying to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_train['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The label has four different classes, each described further in the following table:  \n",
    "\n",
    "|Class|Tag|Description|  \n",
    "|:---:|:---:|:---|  \n",
    "|**-1**|**Anti**|The tweet does not believe in man-made climate change|  \n",
    "|**0**|**Neutral**|The tweet neither supports nor refuses the belief of man-made climate change|   \n",
    "|**1**|**Pro**|The tweet supports the belief of man-made climate change|   \n",
    "|**2**|**News**|The tweet links to factual news about climate change|  \n",
    "\n",
    "Let us have a look at the data associated with each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot1 = plt.figure(figsize=(8.5,5))\n",
    "sns.distplot(df_train['sentiment'],\n",
    "             color='g',kde_kws={'bw':0.1}, bins=100, hist_kws={'alpha': 0.4})\n",
    "plt.title('Distribution graph for different classes');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot2 = df_train['sentiment'].replace({-1: 'Anti',0:'Neutral',1:'Pro',2:'News'}).value_counts().plot(kind='bar',figsize=(8.5,5), color='tan');\n",
    "plt.title('Number of types of comments');\n",
    "plt.xlabel('Comment type');\n",
    "plt.ylabel('Number of comments');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Examining the plot reveals that the majority of the tweets are classified as Pro, i.e. supports the belief of man-made climate change. Since the classes are so unbalanced we expect the classification algorithm to perform better when classifying larger classes ( _Pro_ ) than the smaller classes ( _News, Neutral, Anti_ ).  \n",
    "We have then created a funtion to deal with the uneven distribution of class labels. The function will modify the number of observations for a class(es) we need to resample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Function takes in a dataframe, a class to be resampled, and a class whose observations are to be matched with.\n",
    "#df is the dataframe\n",
    "#class1 is the class that is to be resampled\n",
    "#class2 is the class whose length is used to resample class1\n",
    "def resampling(df, class1, class2):\n",
    "    df_class1= df[df.sentiment==class1]\n",
    "    df_class2 = df[df.sentiment==class2]\n",
    "    df_new= df[df.sentiment!=class1]\n",
    "    resampled = resample(df_class1, replace=False, n_samples=len(df_class2), random_state=27)\n",
    "    df_resampled = pd.concat([resampled, df_new])    \n",
    "    return df_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let us have a look at the most common words associated with each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_Pro = df_train[df_train.sentiment==1]\n",
    "df_News=df_train[df_train.sentiment==2]\n",
    "df_Neutral=df_train[df_train.sentiment==0]\n",
    "df_Anti=df_train[df_train.sentiment==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "k= (' '.join(df_Pro['message']))\n",
    "\n",
    "wordcloud = WordCloud(width = 1000, height = 500,max_words=50).generate(k)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.title('Common words in Pro class');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "l= (' '.join(df_News['message']))\n",
    "\n",
    "wordcloud = WordCloud(width = 1000, height = 500,max_words=50).generate(l)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.title('Common words in News class');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "m= (' '.join(df_Neutral['message']))\n",
    "\n",
    "wordcloud = WordCloud(width = 1000, height = 500,max_words=50).generate(m)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.title('Common words in Neutral class');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n= (' '.join(df_Anti['message']))\n",
    "\n",
    "wordcloud = WordCloud(width = 1000, height = 500,max_words=50).generate(n)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.title('Common words in Anti class');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The `message` column is in its raw form and needs cleaning for the model to be able to process, make analysis and provide predictions for sentiments. Next we will be cleaning the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(df_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There are no missing entries present in the training set. But since an empty string for tweets can also be considered as a missing entry, we need to check for empty strings in the `message` column as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#checking for empty strings\n",
    "blanks = [i for i,lb,tweet in df_train.itertuples() if type(tweet) == str if tweet.isspace()]\n",
    "blanks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The list is empty, indicating that there are no empty strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#checking for duplicates in tweets\n",
    "df_train[df_train.duplicated(subset='message') == True].count()/len(df_train)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "10% of the tweets seem to be duplicated tweets but with different ID's. Since we don't have a significantly large amount of data we will keep these duplicates and check the impact of having them or not having them, when we fit the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Cleaning words in text column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First we'll extract some extra features that can possibly improve the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract all unique news related handles into a list\n",
    "n_temp = [re.findall(r'@[\\w]+',df_train['message'].iloc[i]) for i,x in enumerate(df_train['sentiment']) if x==2]\n",
    "news = [x for x in n_temp if x!=[]]\n",
    "news = list(set(itertools.chain.from_iterable(news)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Function to extract sentiment\n",
    "def sentiment_score(text):\n",
    "    '''docstring'''\n",
    "    \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    s = sid.polarity_scores(text)['compound']\n",
    "    if s<-0.05:\n",
    "        sentiment='negative'\n",
    "    elif s>0.05:\n",
    "        sentiment='positive'\n",
    "    else:\n",
    "        sentiment='neutral'\n",
    "    \n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`#Hashtags` are used to make a statement about something or to make a statement,or conversation around a specific topic trend. In twitter data a lot of hashtags are used and may contain viable information that indicate a certain sentiment towards a specific topic. However hashtags are compressed set of words or sentences and so since it it recorded as 1 word it may be hard for the model to decipher them. So a dictionary,`hashtags`, with all the possible hashtags about climate change has been created with the corresponding decompressed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hashtags = df_train['message'].apply(lambda x: re.findall(r'[#]\\\\w+',x))\n",
    "hashtags = list(set([item for sublist in hashtags for item in sublist])) #extracting all unique #hashtags\n",
    "#exporting twice .one is used for the keys in dictionary the other is used for values(separated words) in dictionary\n",
    "with open('hash_file', 'wb') as fp:\n",
    "    pickle.dump(hashtags, fp)\n",
    "with open('hash_file_clean', 'wb') as fp:\n",
    "    pickle.dump(itemlist, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "*These files are then modified further outside jupyter notebook. DO NOT RUN THIS CODE(above) IT IS JUST FOR ILLUSTRATION OF PROCESS TAKEN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#read in created hashtag text file and create a hashtags dictionary\n",
    "hash_file = [line.rstrip('\\n') for line in open('hash_file.txt')] #keys\n",
    "\n",
    "hash_file_clean = [line.rstrip('\\n') for line in open('hash_file_clean.txt')] #values\n",
    "hashtags = {hash_file[i]: hash_file_clean[i] for i in range(len(hash_file))} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#substitute #hastags with separated words\n",
    "def expand_hashtags(df,column_name):\n",
    "    \"\"\"This funtion takes in a dataframe and text column and returns a dataframe where the hashtag words in the text column\n",
    "    have been expanded into separate words. e.g #iamgreat returns 'i am great' \"\"\"\n",
    "    df[column_name] = df[column_name].str.lower()\n",
    "    for word in hashtags.keys():\n",
    "            df[column_name] = df[column_name].apply(lambda x: re.sub(word,hashtags[word],x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Start cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The `message` column contains some contracted words such as *can't* and *don't*. As part of the cleaning process these words will be replaced with their expanded words that don't contain any ommission. A dictionary `contactions` is created with all the contractions and their corresponding full words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Dictionary of contracted words\n",
    "contractions = {\n",
    "\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we'll\":\"we will\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#replace contracted words with full word\n",
    "df_train['message'] = [' '.join([contractions[w.lower()] if w.lower() in contractions.keys() else w for w in raw.split()]) for raw in df_train['message']]\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#lower case all words to remove noise from Capital words. Capital words may be seen as different from lower case words\n",
    "df_train['message'] = df_train['message'].str.lower()\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Tweets data can also include bad unicode where a person wants so say `Jos√© Flor√©s` or use a `(‚Äî)` but it ends up being `Jos√É¬© Flor√©s` or `√¢‚Ç¨‚Äù` instead. These issues can make it hard for the model to process the data. There is a package in python that takes care of these issues for us. The ftfy(fixed this for you package) takes all the bad unicode and outputs the good unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_train['message'] = df_train['message'].apply(lambda x: fix_text(x))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#removing urls\n",
    "df_train['message'] = df_train['message'].apply(lambda x: re.sub(r'https\\S+','url',x))\n",
    "df_train['message'] = df_train['message'].apply(lambda x: re.sub(r'www\\S+', 'url',x))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The `message` column may also contain emojis üë®üèΩ‚Äçüíª which alone are just symbols and don't have a word association for them. So the emojis will be replaced with the word that describes them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#replace emojis with their word meaning\n",
    "df_train['message'] = df_train['message'].apply(lambda x: emoji.demojize(x))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "With tweets data a lot of shortened words like *abt* are used instead of the full words *about*. This can make it difficult for the model to process these words. We will replace the shortened words with their corresponding full word. A dictionary of all the shortened words with their corresponding full words is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#replace shortened words with full words\n",
    "short = {' BD ': ' Big Deal ',\n",
    " ' abt ':' about ',\n",
    " ' ab ': ' about ',\n",
    " ' fav ': ' favourite ',\n",
    " ' fab ': ' fabulous ',\n",
    " ' smh ': ' shaking my head ',\n",
    " ' u ': ' you ',\n",
    " ' c ': ' see ',\n",
    " ' anon ': ' anonymous ',\n",
    " ' ac ': ' aircon ',\n",
    " ' a/c ': ' aircon ',\n",
    " ' yo ':' year old ',\n",
    " ' n ':' and ',\n",
    " ' nd ':' and ',\n",
    " ' 2 ': ' to ',\n",
    " ' w ': ' with ',\n",
    " ' w/o ': ' without ',\n",
    " ' r ': ' are ',\n",
    " ' rip ':' rest in peace ',\n",
    " ' 4 ' : ' for ',\n",
    "' BF ': ' Boyfriend ',\n",
    "' BRB ': ' Be Right Back ',\n",
    "' BTW ': ' By The Way ',\n",
    "' GF ': ' Girlfriend ',\n",
    "' HBD ': ' Happy Birthday ',\n",
    "' JK ': ' Just Kidding ',\n",
    "' K ':' Okay ',\n",
    "' LMK ': ' Let Me Know ',\n",
    "' LOL ': ' Laugh Out Loud ',\n",
    "' HA ':' laugh ',\n",
    "' MYOB ': ' Mind Your Own Business ',\n",
    "' NBD ': ' No Big Deal ',\n",
    "' NVM ': ' Nevermind ',\n",
    "' Obv ':' Obviously ',\n",
    "' Obvi ':' Obviously ',\n",
    "' OMG ': ' Oh My God ',\n",
    "' Pls ': ' Please ',\n",
    "' Plz ': ' Please ',\n",
    "' Q ': ' Question ', \n",
    "' QQ ': ' Quick Question ',\n",
    "' RLY ': ' Really ',\n",
    "' SRLSY ': ' Seriously ',\n",
    "' TMI ': ' Too Much Information ',\n",
    "' TY ': ' Thank You, ',\n",
    "' TYVM ': ' Thank You Very Much ',\n",
    "' YW ': ' You are Welcome ',\n",
    "' FOMO ': ' Fear Of Missing Out ',\n",
    "' FTFY ': ' Fixed This For You ',\n",
    "' FTW ': ' For The Win ',\n",
    "' FYA ': ' For Your Amusement ',\n",
    "' FYE ': ' For Your Entertainment ',\n",
    "' GTI ': ' Going Through It ',\n",
    "' HTH ': ' Here to Help ',\n",
    "' IRL ': ' In Real Life ',\n",
    "' ICYMI ': ' In Case You Missed It ',\n",
    "' ICYWW ': ' In Case You Were Wondering ',\n",
    "' NBC ': ' Nobody Cares Though ',\n",
    "' NTW ': ' Not To Worry ',\n",
    "' OTD ': ' Of The Day ',\n",
    "' OOTD ': ' Outfit Of The Day ',\n",
    "' QOTD ': ' Quote of the Day ',\n",
    "' FOTD ': ' Find Of the Day ',\n",
    "' POIDH ': ' Pictures Or It Did ntt Happen ',\n",
    "' YOLO ': ' You Only Live Once ',\n",
    "' AFAIK ': ' As Far As I Know ',\n",
    "' DGYF ': ' Dang Girl You Fine ',\n",
    "' FWIW ': ' For What It is Worth ',\n",
    "' IDC ': ' I Do not Care ',\n",
    "' IDK ': ' I Do not Know ',\n",
    "' IIRC ': ' If I Remember Correctly ',\n",
    "' IMHO ': ' In My Honest Opinion ',\n",
    "' IMO ': ' In My Opinion ',\n",
    "' Jelly ': ' Jealous ',\n",
    "' Jellz ': ' Jealous ',\n",
    "' JSYK ': ' Just So You Know ',\n",
    "' LMAO ': ' Laughing My Ass Off ',\n",
    "' LMFAO ': ' Laughing My Fucking Ass Off ',\n",
    "' NTS ': ' Note to Self ',\n",
    "' ROFL ': ' Rolling On the Floor Laughing ',\n",
    "' ROFLMAO ': ' Rolling On the Floor Laughing My Ass Off ',\n",
    "' SMH ': ' Shaking My Head ',\n",
    "' TBH ': ' To Be Honest ',\n",
    "' TL;DR ':  ' Too Long; Did not Read ',\n",
    "' TLDR ':  ' Too Long; Did not Read ',\n",
    "' YGTR ': ' You Got That Right ',\n",
    "' AYKMWTS ': ' Are You Kidding Me With This Shit ',\n",
    "' BAMF ': ' Bad Ass Mother Fucker ',\n",
    "' FFS ': ' For Fuck Sake ',\n",
    "' FML ': ' Fuck My Life ',\n",
    "' HYFR ': ' Hell Yeah Fucking Right ',\n",
    "' IDGAF ': ' I Do not Give A Fuck ',\n",
    "' NFW ': ' No Fucking Way ',\n",
    "' PITA ': ' Pain In The Ass ',\n",
    "' POS ': ' Piece of Shit ',\n",
    "' SOL ': ' Shit Outta Luck ',\n",
    "' STFU ': ' Shut the Fuck Up ',\n",
    "' TF ': ' The Fuck ',\n",
    "' WTF ': ' What The Fuck ',\n",
    "' BFN ': ' Bye For Now ',\n",
    "' CU ': ' See You ',\n",
    "' IC ': ' I see ',\n",
    "' CYL ': ' See You Later ',\n",
    "' GTG ': ' Got to Go ',\n",
    "' OMW ': ' On My Way ',\n",
    "' RN ': ' Right Now ',\n",
    "' TTYL ': ' Talk To You Later ',\n",
    "' TYT ': ' Take Your time ',\n",
    "' CC ': ' Carbon Copy ',\n",
    "' CX ': ' Correction ',\n",
    "' DM ': ' Direct Message ',\n",
    "' FB ': ' Facebook ',\n",
    "' FBF ': ' Flash-Back Friday ',\n",
    "' FF ': ' Follow Friday ',\n",
    "' HT ': ' Tipping my hat ',\n",
    "' H/T ': ' Tipping my hat ',\n",
    "' IG ': ' Instagram ',\n",
    "' Insta ': ' Instagram ',\n",
    "' MT ':' Modified Tweet ',\n",
    "' OH ': ' Overheard ',\n",
    "' PRT ': ' Partial Retweet ',\n",
    "' RT ': ' Retweet ',\n",
    "'rt ' : ' retweet ',\n",
    "' SO ':' Shout Out ',\n",
    "' S/O ': ' Shout Out ',\n",
    "' TBT ': ' Throw-Back Thursday ',\n",
    "' AWOL ': ' Away While Online ',\n",
    "' BFF ': ' Best Friend Forever ',\n",
    "' NSFW ': ' Not Safe For Work ',\n",
    "' OG ': ' Original Gangster ',\n",
    "' PSA ': ' Public Service Announcement ',\n",
    "' PDA ': ' Public Display of Affection '}\n",
    "\n",
    "short = dict((key.lower(), value.lower()) for key,value in short.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#replacing shortened words with full words\n",
    "for word in short.keys():\n",
    "    df_train['message'] = df_train['message'].apply(lambda x: re.sub(word,short[word],x))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#remove twitter non news related handles and @ symbol\n",
    "df_train['message'] = df_train['message'].apply(lambda x: re.sub(r'@','', ' '.join([y for y in x.split() if y not in [z for z in re.findall(r'@[\\w]*',x) if z not in news]])))\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`#Hashtags` are used to make a statement about something or to make a statement,or conversation around a specific topic trend. In twitter data a lot of hashtags are used and may contain viable information that indicate a certain sentiment towards a specific topic. However hashtags are compressed set of words or sentences and so since it it recorded as 1 word it may be hard for the model to decipher them. So a dictionary,`hashtags`, with all the possible hashtags about climate change has been created with the corresponding decompressed words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "???????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#extract all unique hashtags to create separate words out of them\n",
    "# h_temp = [re.findall(r'#[\\w]+',t) for t in df_train['message']]\n",
    "# hashtags = [x for x in h_temp if x!=[]]\n",
    "# hashtags = list(set(itertools.chain.from_iterable(hashtags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#read in created hashtag text file and create a hashtags dictionary\n",
    "# hash_file = [line.rstrip('\\n') for line in open('hash_file.txt')] #keys\n",
    "\n",
    "# hash_file_clean = [line.rstrip('\\n') for line in open('hash_file_clean.txt')] #values\n",
    "# hashtags = {hash_file[i]: hash_file_clean[i] for i in range(len(hash_file))} \n",
    "# hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#substitute #hastags with separated words\n",
    "# def expand_hashtags(df,column_name):\n",
    "#     \"\"\"This funtion takes in a dataframe and text column and returns a dataframe where the hashtag words in the text column\n",
    "#     have been expanded into separate words. e.g #iamgreat returns 'i am great' \"\"\"\n",
    "    \n",
    "#     for word in hashtags.keys():\n",
    "#             df[column_name] = df[column_name].apply(lambda x: re.sub(word,hashtags[word],x))\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df_train = expand_hashtags(df_train,'message')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Punctuation...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#remove punctuation (except ! for sentiment analysis)\n",
    "df_train['message'] = df_train['message'].apply(lambda x: re.sub(r\"[^A-Za-z! ]*\",'',x))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#remove vowels repeated at least 3 times ex. Coooool > Cool\n",
    "df_train['message'] = df_train['message'].apply(lambda x: re.sub(r'([aeiou])\\1+', r'\\1\\1', x))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#replace sequence of 'h' and 'a', as well as 'lol' with 'laugh'\n",
    "df_train['message'] = df_train['message'].apply(lambda x: re.sub(r'ha([ha])*', r'laugh', x))\n",
    "df_train['message'] = df_train['message'].apply(lambda x: re.sub(r'lol([ol])*', r'laugh', x))\n",
    "df_train['message'] = df_train['message'].apply(lambda x: re.sub(r'lo([o])*l', r'laugh', x))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#add sentiment\n",
    "df_train['message'] = df_train['message'].apply(lambda x: x + ' ' + sentiment_score(x))  \n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Tweets data can have alot of noise in it from the type of language, style and grammer used, that the computer may find hard to process. These issues have to be dealt with in order for the model to be abe to process the text data and make predictions. A function(`cleanup(df,column_name) `) is created that handles all these issues:\n",
    "\n",
    "We will work with lower cased data to remove noise from capitalised words as the computer may see uppercased words as different from lower cased words.\n",
    "\n",
    "Tweets data can also include bad unicode where a person wants so say `Jos√© Flor√©s` or use a `(‚Äî)` but it ends up being `Jos√É¬© Flor√©s` or `√¢‚Ç¨‚Äù` instead. These issues can make it hard for the model to process the data. There is a package in python that takes care of these issues for us. The ftfy(fixed this for you package) takes all the bad unicode and outputs the good unicode\n",
    "\n",
    "The `message` column contains some contracted words such as *can't* and *don't*. As part of the cleaning process these words will be replaced with their expanded words that don't contain any ommission. \n",
    "\n",
    "The `message` column may also contain emojis üë®üèΩ‚Äçüíª which alone are just symbols and don't have a word association for them. So the emojis will be replaced with the word that describes them.\n",
    "\n",
    "With tweets data a lot of shortened words like *abt* are used instead of the full words *about*. This can make it difficult for the model to process these words. We will replace the shortened words with their corresponding full word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cleanup(raw):\n",
    "    \"\"\"this function takes in text data  as a string and returns a modified string.\n",
    "    The text gets modified by being; lower cased,removing urls,removing bad unicode,replacing contractions,\n",
    "    replacing emojis with words and replacing shortened words\"\"\"\n",
    "    \n",
    "    #convert to lowercase\n",
    "    raw = raw.lower()\n",
    "    \n",
    "    #fix strange characters\n",
    "    raw = fix_text(raw)\n",
    "    \n",
    "    #removing urls\n",
    "    raw = re.sub(r'https\\S+','url',raw)\n",
    "    raw = re.sub(r'www\\S+', 'url',raw)\n",
    "    \n",
    "    #replace emojis with their word meaning\n",
    "    raw = emoji.demojize(raw)\n",
    "\n",
    "    #remove twitter non news related handles and @ symbol\n",
    "    raw = ' '.join([y for y in raw.split() if y not in [x for x in re.findall(r'@[\\w]*',raw) if x not in news]])\n",
    "    \n",
    "    #remove punctuation (except ! for sentiment analysis)\n",
    "    raw = re.sub(r\"[^A-Za-z!' ]*\",'',raw)\n",
    "    \n",
    "    #remove vowels repeated at least 3 times ex. Coooool > Cool\n",
    "    raw = re.sub(r'([aeiou])\\1+', r'\\1\\1', raw)\n",
    "    \n",
    "    #replace sequence of 'h' and 'a', as well as 'lol' with 'laugh'\n",
    "    raw = re.sub(r'ha([ha])*', r'laugh', raw)\n",
    "    raw = re.sub(r\"lol([ol])*\", r'laugh', raw)\n",
    "    raw = re.sub(r\"lo([o])*l\", r'laugh', raw)\n",
    "    \n",
    "    #add sentiment\n",
    "    raw = raw + ' ' + sentiment_score(raw)\n",
    "    \n",
    "    return raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#replace contracted words with full word\n",
    "df_test['message'] = [' '.join([contractions[w.lower()] if w.lower() in contractions.keys() else w for w in raw.split()]) for raw in df_test['message']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#replacing shortened words with full words\n",
    "for word in short.keys():\n",
    "    df_test['message'] = df_test['message'].apply(lambda x: re.sub(word,short[word],x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_test['message'] = df_test['message'].apply(lambda x: cleanup(x))\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Check for misspelled words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "spell = SpellChecker() \n",
    "# check for misspelled words\n",
    "misspelled = df_train['message'].apply(lambda x: spell.unknown(x))\n",
    "misspelled.isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Split data into response and predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y = df_train['sentiment']\n",
    "X = df_train['message']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Split data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Build pipelines to vectorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Write class that has object that tokenizes text data AND stems the tokens\n",
    "class StemAndTokenize:\n",
    "    def __init__(self):\n",
    "        self.ss = SnowballStemmer('english')\n",
    "    def __call__(self, doc):\n",
    "        return [self.ss.stem(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create pipeline for Logistic Regression:\n",
    "lr = Pipeline([('tfidf', TfidfVectorizer(tokenizer=StemAndTokenize())),\n",
    "               ('lr', LogisticRegression())\n",
    "              ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Na√Øve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create pipeline for Na√Øve Bayes:\n",
    "nb = Pipeline([('tfidf', TfidfVectorizer(tokenizer=StemAndTokenize())),\n",
    "               ('nb', MultinomialNB())\n",
    "              ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Linear SVM (Support Vector Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create pipeline for SVM:\n",
    "Lsvm = Pipeline([('tfidf', TfidfVectorizer(tokenizer=StemAndTokenize())),\n",
    "               ('Lsvm', LinearSVC())\n",
    "              ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create pipeline for Random Forest:\n",
    "rf = Pipeline([('tfidf', TfidfVectorizer(tokenizer=StemAndTokenize())),\n",
    "               ('rf', RandomForestClassifier())\n",
    "              ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### KNN (K Nearest Neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create pipeline for KNN:\n",
    "knn = Pipeline([('tfidf', TfidfVectorizer(tokenizer=StemAndTokenize())),\n",
    "               ('knn', KNeighborsClassifier())\n",
    "              ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create pipeline for Neural Networks:\n",
    "nn = Pipeline([('tfidf', TfidfVectorizer(tokenizer=StemAndTokenize())),\n",
    "               ('nn', MLPClassifier())\n",
    "              ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# optimize models by tuning parameters (GridSearch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Fitting the Logistic Regression model\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Na√Øve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Fitting the Na√Øve Bayes model\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Linear SVM (Support Vector Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Fitting the SVM model\n",
    "Lsvm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Fitting the Random Forest model\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### KNN (K Nearest Neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Fitting the KNN model\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Fitting the Neural Networks model\n",
    "nn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Form a prediction set for the Logistic Regression model\n",
    "pred_lr = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Na√Øve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Form a prediction set for the Na√Øve Bayes model\n",
    "pred_nb = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Linear SVM (Support Vector Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Form a prediction set for the Linear SVM model\n",
    "pred_Lsvm = Lsvm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Form a prediction set for the Random Forest model\n",
    "pred_rf = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### KNN (K Nearest Neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Form a prediction set for the KNN model\n",
    "pred_knn = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Form a prediction set for the Neural Network model\n",
    "pred_nn = nn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Evaluate model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Confusion Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "labels = ['2: News', '1: Pro', '0: Neutral', '-1: Anti']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(data=confusion_matrix(y_test, pred_lr), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Na√Øve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(data=confusion_matrix(y_test, pred_nb), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Linear SVM (Support Vector Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(data=confusion_matrix(y_test, pred_Lsvm), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(data=confusion_matrix(y_test, pred_rf), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### KNN (K Nearest Neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(data=confusion_matrix(y_test, pred_knn), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(data=confusion_matrix(y_test, pred_nn), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Classification Report from Logistic Regression Model')\n",
    "print(classification_report(y_test, pred_lr, target_names=['2: News', '1: Pro', '0: Neutral', '-1: Anti']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Na√Øve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Classification Report from Na√Øve Model')\n",
    "print(classification_report(y_test, pred_nb, target_names=['2: News', '1: Pro', '0: Neutral', '-1: Anti']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Linear SVM (Support Vector Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Classification Report from Linear SVM (Support Vector Machine) Model')\n",
    "print(classification_report(y_test, pred_Lsvm, target_names=['2: News', '1: Pro', '0: Neutral', '-1: Anti']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Classification Report from Random Forest Model')\n",
    "print(classification_report(y_test, pred_rf, target_names=['2: News', '1: Pro', '0: Neutral', '-1: Anti']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### KNN (K Nearest Neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Classification Report from KNN(K Nearest Neighbours) Model')\n",
    "print(classification_report(y_test, pred_knn, target_names=['2: News', '1: Pro', '0: Neutral', '-1: Anti']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Classification Report from Neural Networks Model')\n",
    "print(classification_report(y_test, pred_nn, target_names=['2: News', '1: Pro', '0: Neutral', '-1: Anti']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Table for all models with only f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#lr.predict(X_test) -> use predictions created above\n",
    "data = {'Model':['Logistic Regression','Na√Øve Bayes','Linear SVM','Random Forest','KNN','Neural Network'],\n",
    "        'F1_score' :[f1_score(y_test, lr.predict(X_test)),\n",
    "       f1_score(y_test, pred_lr),\n",
    "       f1_score(y_test, pred_nb)\n",
    "       f1_score(y_test, pred_Lsvm),\n",
    "       f1_score(y_test, pred_knn),\n",
    "       f1_score(y_test, pred_nn)]}\n",
    "\n",
    "pd.DataFrame(data=data, columns=['Model F1_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Lsvm\n",
    "model_save_path = \"Linear_SVM_base_model.pkl\"\n",
    "with open(model_save_path,'wb') as file:\n",
    "    pickle.dump(model,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Log parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f1 = f1_score(y_test, pred_Lsvm, average='weighted')\n",
    "recall = recall_score(y_test, pred_Lsvm, average='weighted')\n",
    "precision = precision_score(y_test, pred_Lsvm, average='weighted')\n",
    "accuracy = accuracy_score(y_test, pred_Lsvm)\n",
    "confusion_mat = confusion_matrix(y_test, pred_Lsvm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#these will be logged to your sklearn-demos project on Comet.ml\n",
    "# (FINAL MODEL after gridsearch best parms)\n",
    "params={\"random_state\":42,\n",
    "        \"test_size\":0.2\n",
    "        \"model_type\":\"Linear SVM\"\n",
    "       }\n",
    "\n",
    "metrics = {\"f1\":f1,\n",
    "           \"recall\":recall,\n",
    "           \"precision\":precision,\n",
    "           \"accuracy\":accuracy\n",
    "            }\n",
    "\n",
    "# exp.log_dataset_hash(X_train_scaled)\n",
    "exp.log_parameters(params)\n",
    "exp.log_metrics(metrics)\n",
    "\n",
    "experiment.log_confusion_matrix(labels=['2: News', '1: Pro', '0: Neutral', '-1: Anti'],matrix=confusion_mat)\n",
    "\n",
    "experiment.log_figure(figure=plot1,figure_name='Distribution graph for different classes')\n",
    "experiment.log_figure(figure=plot2,figure_name='Number of types of comments')\n",
    "\n",
    "experiment.log_model(name='Linear SVM - base model', file_or_folder='pickled file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### End experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
